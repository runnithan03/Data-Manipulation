X_home
#To avoid a singular (non-invertible) matrix, which cannot be used in regression analysis due to perfect multicollinearity,
#one team's column is typically dropped. This team serves as the reference category against which the strengths of all other teams are compared.
#In the provided example, the first team alphabetically (e.g., Arsenal) is dropped.
# Dropping the first team (e.g., Arsenal)
X <- X[,-1]
dim(X)
# Combining matrices and converting to a data frame
X <- data.frame(X_home + X_away)
#To avoid a singular (non-invertible) matrix, which cannot be used in regression analysis due to perfect multicollinearity,
#one team's column is typically dropped. This team serves as the reference category against which the strengths of all other teams are compared.
#In the provided example, the first team alphabetically (e.g., Arsenal) is dropped.
# Dropping the first team (e.g., Arsenal)
X <- X[,-1]
dim(X)
#(b) Creating the Vector y and Combining with X
# Assuming 'FTR' is the column indicating full-time result with 'H' for home wins
y <- as.integer(data$FTR == 'H')
y
# Combining X and y
matchdata <- cbind(X, HomeWin=y)
#(c) Fitting the Bradley-Terry Model
model <- glm(HomeWin ~ ., family=binomial(link="logit"), data=matchdata)
summary(model)
sort(model$coefficients)
sort(model$coefficients, descending = TRUE)
sort(model$coefficients, decreasing  = TRUE)
#(c) Fitting the Bradley-Terry Model
model <- glm(HomeWin ~0+., family=binomial(link="logit"), data=matchdata)
summary(model)
sort(model$coefficients, decreasing  = TRUE)
View(matchdata)
# Combining X and y
matchdata <- cbind( HomeWin=y, X)
#(c) Fitting the Bradley-Terry Model
model <- glm(HomeWin ~0+., family=binomial(link="logit"), data=matchdata)
#the use of 0+. means you don't include the intercept in the model
summary(model)
sort(model$coefficients, decreasing  = TRUE)
View(matchdata)
#(d) Dispersion Parameter φ
# Checking for overdispersion
phi_est <- model$deviance / model$df.residual
phi_est  # Close to 1 suggests no overdispersion
phi_est  # Close to 1 suggests no overdispersion
#(e) Modelling Home Team Advantage
model_home_advantage <- glm(HomeWin ~ . + 1, family=binomial(link="logit"), data=matchdata)
summary(model_home_advantage)
sort(model$coefficients, decreasing  = TRUE)
#(e) Modelling Home Team Advantage
model_home_advantage <- glm(HomeWin ~., family=binomial(link="logit"), data=matchdata)
summary(model_home_advantage)
#now we inclulde the intercept in the model
summary(model_home_advantage)
sort(model$coefficients, decreasing  = TRUE)
#now we include the intercept in the model
summary(model_home_advantage)
#the use of 0+ means you don't include the intercept in the model
summary(model)
#(c) Fitting the Bradley-Terry Model
model <- glm(HomeWin ~., family=binomial(link="logit"), data=matchdata)
#the use of 0+ means you don't include the intercept in the model
summary(model)
sort(model$coefficients, decreasing  = TRUE)
#the use of 0+ means you don't include the intercept in the model
summary(model)
#the use of 0+ means you don't include the intercept in the model
summary(model)
View(X)
View(matchdata)
#(e) Modelling Home Team Advantage
model_home_advantage <- glm(HomeWin ~., family=binomial(link="logit"), data=matchdata)
#now we include the intercept in the model because the intercept tells you the home team advantage
#so, the intercept estimate is negative for this sample indicating if anything, there is a home team disadcantage
summary(model_home_advantage)
#(f) Confidence Region for Team Strengths
# Assuming 'ManCity' and 'Liverpool' are coefficient names in your model
cov_matrix <- vcov(model)
teams <- c("HomeTeamLiverpool","HomeTeamMan.City")
cov_teams <- cov_matrix[teams,teams]
coef_man_city <- cov_teams[2,2]
coef_liverpool <- cov_teams[1,1]
library(ellipse)
# Generating ellipse points for the 95% confidence region
ellipse_points <- ellipse(cov_matrix, level = 0.95, centre = c(coef_man_city, coef_liverpool))
# Generating ellipse points for the 95% confidence region
ellipse_points <- ellipse(cov_matrix, level = 0.95, centre = c(coef_man_city, coef_liverpool))
# Plotting the ellipse
plot(ellipse_points, type = "l", xlab = "Strength Man City", ylab = "Strength Liverpool",
main = "95% Confidence Region for Man City and Liverpool Strengths")
points(coef_man_city, coef_liverpool, pch = 19, col = "red")
#(a) Constructing the Design Matrix
data <- read.csv("FootballData.csv")
# Assuming 'data' is your loaded data frame and it has columns 'HomeTeam' and 'AwayTeam'
data$HomeTeam <- as.factor(data$HomeTeam)
data$AwayTeam <- as.factor(data$AwayTeam)
# Creating model matrices
#the model.matrix() function creates binary (0/1) indicator variables (dummy variables) for each level of the factor,
#except for the first level which is omitted to avoid multicollinearity (this is known as the "reference level").
X_home <- model.matrix(~ HomeTeam - 1, data=data)
X_away <- model.matrix(~ AwayTeam - 1, data=data) * -1
# Combining matrices and converting to a data frame
X <- data.frame(X_home + X_away)
#To avoid a singular (non-invertible) matrix, which cannot be used in regression analysis due to perfect multicollinearity,
#one team's column is typically dropped. This team serves as the reference category against which the strengths of all other teams are compared.
#In the provided example, the first team alphabetically (e.g., Arsenal) is dropped.
# Dropping the first team (e.g., Arsenal)
X <- X[,-1]
dim(X)
#(b) Creating the Vector y and Combining with X
# Assuming 'FTR' is the column indicating full-time result with 'H' for home wins
y <- ifelse(Football$FTR == "H", 1, 0)
# Combining X and y
matchdata <- cbind( HomeWin=y, X)
#(b) Creating the Vector y and Combining with X
# Assuming 'FTR' is the column indicating full-time result with 'H' for home wins
y <- ifelse(Football$FTR == "H", 1, 0)
#(b) Creating the Vector y and Combining with X
# Assuming 'FTR' is the column indicating full-time result with 'H' for home wins
y <- ifelse(data$FTR == "H", 1, 0)
# Combining X and y
matchdata <- cbind( HomeWin=y, X)
#(c) Fitting the Bradley-Terry Model
model <- glm(HomeWin ~0+., family=binomial(link="logit"), data=matchdata)
#the use of 0+ means you don't include the intercept in the model
summary(model)
sort(model$coefficients, decreasing  = TRUE)
#(d) Dispersion Parameter φ
# Checking for overdispersion
phi_est <- model$deviance / model$df.residual
phi_est  # Close to 1 suggests no overdispersion
#(e) Modelling Home Team Advantage
model_home_advantage <- glm(HomeWin ~., family=binomial(link="logit"), data=matchdata)
summary(model_home_advantage)
#(f) Confidence Region for Team Strengths
# First, get the full Fisher information matrix
Finv_betahat <- summary(model_home_advantage)$cov.scaled
# For this confidence region, we just want the submatrix involving
# Man City and Liverpool.
# Also, we want the inverse for the Mahalanobis distance
F_lmc <- solve(Finv_betahat[c(11, 13), c(11, 13)])
F_lmc
# Get the MLEs for these two teams
betahat_lmc <- coef(model_home_advantage)[c(11, 13)]
# Let's setup a grid of strengths which we'll check the Mahalanobis distance
# against the chi-squared critical value
liverpool <- seq(-4, 4, length.out = 300)
man_city <- seq(-4, 4, length.out = 300)
# Use the outer function to evaluate over a grid
HessCR <- outer(liverpool, man_city,
Vectorize(function(beta_l, beta_mc) {
beta <- c(beta_l, beta_mc)
t(betahat_lmc - beta) %*% F_lmc %*% (betahat_lmc - beta)
}
))
# The image function now lets us colour the part we're interested in
image(liverpool, man_city, HessCR > qchisq(0.95, 2))
# and mark the location of the MLE for reference
points(betahat_lmc[1], betahat_lmc[2], pch = 3, col = "black")
# You could also add the individual confidence intervals calculated by R as
# horizontal and vertical lines, but note it is doing something called profiling
# to get the confidence interval accounting for all other variables so you will
# notice a small discrepancy between the marginal and the extremes of the joint
abline(h = confint(model_home_advantage, "Man.City"))
abline(v = confint(model_home_advantage, "Liverpool"))
#(i) Model Accuracy with Training and Testing Data
# Splitting the data
train_data <- matchdata[1:150, ]
test_data <- matchdata[-(1:150), ]
# Fit the model on training data and predict on test data
model_train <- glm(HomeWin ~ ., family=binomial(link="logit"), data=train_data)
predictions <- predict(model_train, newdata=test_data, type="response")
predicted_classes <- ifelse(predictions > 0.5, 1, 0)
# Calculate accuracy
accuracy <- mean(predicted_classes == test_data$HomeWin) * 100
accuracy
##### (i) #####
# Split the data up
training <- matchdata[1:150,]
testing <- matchdata[-(1:150),]
# Fit the model on the training data only
fit4 <- glm(y ~ ., training, family = binomial(link=logit))
##### (b) #####
# Finally, create vector y of home team wins and make data frame
y <- ifelse(Football$FTR == "H", 1, 0)
# Load the data
Football <- read.csv("FootballData.csv")
##### (b) #####
# Finally, create vector y of home team wins and make data frame
y <- ifelse(Football$FTR == "H", 1, 0)
# Fit the model on the training data only
fit4 <- glm(y ~ ., training, family = binomial(link=logit))
##### (i) #####
# Split the data up
training <- matchdata[1:150,]
testing <- matchdata[-(1:150),]
# Fit the model on the training data only
fit4 <- glm(y ~ ., training, family = binomial(link=logit))
summary(fit4)
# Fit the model on the training data only
fit4 <- glm(y ~ ., training, family = binomial(link=logit))
# Fit the model on the training data only
fit4 <- glm(Homewin ~ ., training, family = binomial(link=logit))
# Fit the model on the training data only
fit4 <- glm(HomeWin ~ ., training, family = binomial(link=logit))
summary(fit4)
# Predict on the testing
pred <- predict(fit4, testing, type = "response")
# Now produce a table, where we want to compare the truth to our prediction
res <- table(truth = testing$y,
prediction = ifelse(pred > 0.5, 1, 0))
res
# Now produce a table, where we want to compare the truth to our prediction
res <- table(truth = testing$y,
prediction = ifelse(pred > 0.5, 1, 0))
# Load the data
Football <- read.csv("FootballData.csv")
View(Football)
##### (a) #####
# Helpfully, R splits factors into columns, so we can use it to create an
# indicator variable for each team both when home and away
X_home <- model.matrix( ~ 0+HomeTeam, data=Football)
X_away <- model.matrix( ~ 0+AwayTeam, data=Football)
View(X_home)
View(X_away)
# X_home has 1 when home, and X_away has 1 when away, so we want the difference
# Convert the difference into a data frame so variables (columns) are named
X <- as.data.frame(X_home - X_away)
nrow(X)
ncol(X)
# Optionally, we can pretty up the names by removing the "HomeTeam" part
# at the start
names(X) <- substring(names(X), 9)
# Use the "stringr" package to replace the empty space in team names with a dot.
library(stringr)
names(X) <- str_replace_all(names(X), c(" " = ".", "," = ""))
# To avoid a singular solution due to collinearity of indicators, we simply
# drop the first team, Arsenal
X <- X[,-1]
##### (b) #####
# Finally, create vector y of home team wins and make data frame
y <- ifelse(Football$FTR == "H", 1, 0)
matchdata <- cbind(y = y, X)
View(matchdata)
##### (c) #####
# Fit Bradley-Terry model via logistic regression
fit1 <- glm(y ~ 0+., matchdata, family = binomial(link=logit))
summary(fit1)
# Look at the coefficients sorted (remember that Arsenal is the reference team
# so its estimate is 0, and any teams with a estimate greater than 0 are stronger
# than Arsenal and any teams with a estimate less than 0 are weaker than Arsenal
# in terms of a high home-win probability)
sort(coef(fit1), decreasing = TRUE)
# Tottenham seems to be stronger under the model than Man City.
# Notice that looking at the away games for Tottenham and Man City,
# they achieved several draws which count as a 'victory' when playing away
# with this model. Hence, probability of away victories likely inflated.
Football[Football$AwayTeam=="Tottenham", 2:7]
Football[Football$AwayTeam=="Man City", 2:7]
##### (d) #####
# We expect a logistic regression to have dispersion parameter 1
# Estimating it from the model fit, we get
fit1$deviance / fit1$df.residual
##### (e) #####
# A home team advantage could be modelled as a constant multiplicative effect
# on the odds of a home team win. This is exactly the effect an intercept term
# would have.
# So, if we drop the 0+ to allow R to put in an intercept:
fit2 <- glm(y ~ ., matchdata, family = binomial(link=logit))
summary(fit2)
# Intercept is actually negative! However, in fact there's quite a lot of
# uncertainty here, so it's just saying there isn't enough data for us to
# distinguish it
confint(fit2, "(Intercept)")
##### (f) #####
# First, get the full Fisher information matrix
Finv_betahat <- summary(fit2)$cov.scaled
# For this confidence region, we just want the submatrix involving
# Man City and Liverpool.
# Also, we want the inverse for the Mahalanobis distance
F_lmc <- solve(Finv_betahat[c(11, 13), c(11, 13)])
F_lmc
# Get the MLEs for these two teams
betahat_lmc <- coef(fit2)[c(11, 13)]
# Let's setup a grid of strengths which we'll check the Mahalanobis distance
# against the chi-squared critical value
liverpool <- seq(-4, 4, length.out = 300)
man_city <- seq(-4, 4, length.out = 300)
# Use the outer function to evaluate over a grid
HessCR <- outer(liverpool, man_city,
Vectorize(function(beta_l, beta_mc) {
beta <- c(beta_l, beta_mc)
t(betahat_lmc - beta) %*% F_lmc %*% (betahat_lmc - beta)
}
))
# The image function now lets us colour the part we're interested in
image(liverpool, man_city, HessCR > qchisq(0.95, 2))
# and mark the location of the MLE for reference
points(betahat_lmc[1], betahat_lmc[2], pch = 3, col = "black")
# You could also add the individual confidence intervals calculated by R as
# horizontal and vertical lines, but note it is doing something called profiling
# to get the confidence interval accounting for all other variables so you will
# notice a small discrepancy between the marginal and the extremes of the joint
abline(h = confint(fit2, "Man.City"))
abline(v = confint(fit2, "Liverpool"))
##### (g) #####
# We need to compute the model with the three teams fixed to have the same
# coefficient. To do this, simply tell R to remove the individual predictors
# and insert a new predictor formed from all three
fit3 <- glm(y ~ . - Everton - Burnley - Sheffield.United
+ I(Everton + Burnley + Sheffield.United),
matchdata, family = binomial(link=logit))
summary(fit3)
# In logistic regression, dispersion is 1, so the likelihood ratio test
# statistic can be found by just taking the difference of the deviances
fit3$deviance - fit2$deviance
# Difference in degrees of freedom (should be 2 as we've replaced 3 coefficients
# with 1)
fit3$df.residual - fit2$df.residual
# Test is against chi-sq(v=2)
qchisq(0.95, 2)
##### (h) #####
# We need to predict a new match.  Easiest to pull a row from X, zero out and
# then set the home/away teams we need
new_match <- X[1,]
new_match[,1:19] <- 0
new_match$Man.City <- +1
new_match$Chelsea <- -1
View(new_match)
# Probability of Chelsea win is probability of away win,
# so 1 minus predicted probability of home win
1-predict(fit3, new_match, type = "response")
##### (i) #####
# Split the data up
training <- matchdata[1:150,]
testing <- matchdata[-(1:150),]
# Fit the model on the training data only
fit4 <- glm(HomeWin ~ ., training, family = binomial(link=logit))
# Fit the model on the training data only
fit4 <- glm(HomeWin ~ ., training, family = binomial(link=logit))
# Fit the model on the training data only
fit4 <- glm(HomeWin ~ ., training, family = binomial(link=logit))
# Fit the model on the training data only
fit4 <- glm(HomeWin ~ ., training, family = binomial(link=logit))
summary(fit4)
# Fit the model on the training data only
fit4 <- glm(HomeWin ~ ., training, family = binomial(link=logit))
summary(fit4)
# Predict on the testing
pred <- predict(fit4, testing, type = "response")
# Now produce a table, where we want to compare the truth to our prediction
res <- table(truth = testing$y,
prediction = ifelse(pred > 0.5, 1, 0))
res
# Hence overall accuracy (%) is
(res[1,1]+res[2,2])/sum(res)*100
#(h) Probability of Chelsea Winning Against Man City
# Calculate the log-odds and convert to probability
log_odds <- coef(model)['Intercept'] + coef(model)['Chelsea'] - coef(model)['ManCity']
prob_win <- exp(log_odds) / (1 + exp(log_odds))
prob_win
# Load the data
Football <- read.csv("FootballData.csv")
View(Football)
##### (a) #####
# Helpfully, R splits factors into columns, so we can use it to create an
# indicator variable for each team both when home and away
X_home <- model.matrix( ~ 0+HomeTeam, data=Football)
X_away <- model.matrix( ~ 0+AwayTeam, data=Football)
View(X_home)
View(X_away)
# X_home has 1 when home, and X_away has 1 when away, so we want the difference
# Convert the difference into a data frame so variables (columns) are named
X <- as.data.frame(X_home - X_away)
nrow(X)
ncol(X)
# Optionally, we can pretty up the names by removing the "HomeTeam" part
# at the start
names(X) <- substring(names(X), 9)
# Use the "stringr" package to replace the empty space in team names with a dot.
library(stringr)
names(X) <- str_replace_all(names(X), c(" " = ".", "," = ""))
# To avoid a singular solution due to collinearity of indicators, we simply
# drop the first team, Arsenal
X <- X[,-1]
##### (b) #####
# Finally, create vector y of home team wins and make data frame
y <- ifelse(Football$FTR == "H", 1, 0)
matchdata <- cbind(y = y, X)
View(matchdata)
View(Football)
View(matchdata)
##### (c) #####
# Fit Bradley-Terry model via logistic regression
fit1 <- glm(y ~ 0+., matchdata, family = binomial(link=logit))
summary(fit1)
# Look at the coefficients sorted (remember that Arsenal is the reference team
# so its estimate is 0, and any teams with a estimate greater than 0 are stronger
# than Arsenal and any teams with a estimate less than 0 are weaker than Arsenal
# in terms of a high home-win probability)
sort(coef(fit1), decreasing = TRUE)
# Tottenham seems to be stronger under the model than Man City.
# Notice that looking at the away games for Tottenham and Man City,
# they achieved several draws which count as a 'victory' when playing away
# with this model. Hence, probability of away victories likely inflated.
Football[Football$AwayTeam=="Tottenham", 2:7]
Football[Football$AwayTeam=="Man City", 2:7]
##### (d) #####
# We expect a logistic regression to have dispersion parameter 1
# Estimating it from the model fit, we get
fit1$deviance / fit1$df.residual
##### (e) #####
# A home team advantage could be modelled as a constant multiplicative effect
# on the odds of a home team win. This is exactly the effect an intercept term
# would have.
# So, if we drop the 0+ to allow R to put in an intercept:
fit2 <- glm(y ~ ., matchdata, family = binomial(link=logit))
summary(fit2)
# Intercept is actually negative! However, in fact there's quite a lot of
# uncertainty here, so it's just saying there isn't enough data for us to
# distinguish it
confint(fit2, "(Intercept)")
##### (f) #####
# First, get the full Fisher information matrix
Finv_betahat <- summary(fit2)$cov.scaled
# For this confidence region, we just want the submatrix involving
# Man City and Liverpool.
# Also, we want the inverse for the Mahalanobis distance
F_lmc <- solve(Finv_betahat[c(11, 13), c(11, 13)])
F_lmc
# Get the MLEs for these two teams
betahat_lmc <- coef(fit2)[c(11, 13)]
betahat_lmc
# Let's setup a grid of strengths which we'll check the Mahalanobis distance
# against the chi-squared critical value
liverpool <- seq(-4, 4, length.out = 300)
man_city <- seq(-4, 4, length.out = 300)
liverpool
# Use the outer function to evaluate over a grid
HessCR <- outer(liverpool, man_city,
Vectorize(function(beta_l, beta_mc) {
beta <- c(beta_l, beta_mc)
t(betahat_lmc - beta) %*% F_lmc %*% (betahat_lmc - beta)
}
))
# The image function now lets us colour the part we're interested in
image(liverpool, man_city, HessCR > qchisq(0.95, 2))
# and mark the location of the MLE for reference
points(betahat_lmc[1], betahat_lmc[2], pch = 3, col = "black")
# You could also add the individual confidence intervals calculated by R as
# horizontal and vertical lines, but note it is doing something called profiling
# to get the confidence interval accounting for all other variables so you will
# notice a small discrepancy between the marginal and the extremes of the joint
abline(h = confint(fit2, "Man.City"))
abline(v = confint(fit2, "Liverpool"))
fit2
##### (g) #####
# We need to compute the model with the three teams fixed to have the same
# coefficient. To do this, simply tell R to remove the individual predictors
# and insert a new predictor formed from all three
fit3 <- glm(y ~ . - Everton - Burnley - Sheffield.United
+ I(Everton + Burnley + Sheffield.United),
matchdata, family = binomial(link=logit))
summary(fit3)
# In logistic regression, dispersion is 1, so the likelihood ratio test
# statistic can be found by just taking the difference of the deviances
fit3$deviance - fit2$deviance
# Difference in degrees of freedom (should be 2 as we've replaced 3 coefficients
# with 1)
fit3$df.residual - fit2$df.residual
# Test is against chi-sq(v=2)
qchisq(0.95, 2)
##### (h) #####
# We need to predict a new match.  Easiest to pull a row from X, zero out and
# then set the home/away teams we need
new_match <- X[1,]
.new_match[,1:19] <- 0
new_match[,1:19] <- 0
new_match$Man.City <- +1
new_match$Chelsea <- -1
View(new_match)
# Probability of Chelsea win is probability of away win,
# so 1 minus predicted probability of home win
1-predict(fit3, new_match, type = "response")
##### (i) #####
# Split the data up
training <- matchdata[1:150,]
testing <- matchdata[-(1:150),]
# Fit the model on the training data only
fit4 <- glm(y ~ ., training, family = binomial(link=logit))
summary(fit4)
# Predict on the testing
pred <- predict(fit4, testing, type = "response")
# Now produce a table, where we want to compare the truth to our prediction
res <- table(truth = testing$y,
prediction = ifelse(pred > 0.5, 1, 0))
res
# Hence overall accuracy (%) is
(res[1,1]+res[2,2])/sum(res)*100
# Note: not very much high prediction accuracy here, perhaps we should be a bit
# Note: not very much high prediction accuracy here, perhaps we should be a bit
# more careful in choosing the cut-off point (here 0.5) for prediction of win
##### (j) #####
##### (j) #####
# To use a rescaled Bionamial model, one can consider the number of goals
##### (j) #####
# To use a rescaled Bionamial model, one can consider the number of goals
# scored in a match. For this, one can define the response variable to be
##### (j) #####
# To use a rescaled Bionamial model, one can consider the number of goals
# scored in a match. For this, one can define the response variable to be
# the number of goals by home team (in column FTHG) while "n" would be the
